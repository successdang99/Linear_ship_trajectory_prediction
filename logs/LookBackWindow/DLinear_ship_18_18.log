Args in experiment:
Namespace(activation='gelu', batch_size=32, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='Ship', data_path='.csv', dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=4, factor=1, features='M', freq='h', gpu=0, individual=False, is_training=1, itr=1, label_len=18, learning_rate=0.05, loss='mse', lradj='type1', model='DLinear', model_id='ship_18_18', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=18, root_path='./dataset/Ship/', seq_len=18, target='OT', test_flop=False, train_epochs=10, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ship_18_18_DLinear_Ship_ftM_sl18_ll18_pl18_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 9144
val 1291
test 1453
	iters: 100, epoch: 1 | loss: 0.0065163
	speed: 0.0073s/iter; left time: 20.1593s
	iters: 200, epoch: 1 | loss: 0.0053071
	speed: 0.0042s/iter; left time: 11.2590s
Epoch: 1 cost time: 1.5641858577728271
Epoch: 1, Steps: 285 | Train Loss: 0.0110488 Vali Loss: 0.0051926 Test Loss: 0.0052511
Validation loss decreased (inf --> 0.005193).  Saving model ...
Updating learning rate to 0.05
	iters: 100, epoch: 2 | loss: 0.0049561
	speed: 0.0210s/iter; left time: 51.6886s
	iters: 200, epoch: 2 | loss: 0.0057797
	speed: 0.0041s/iter; left time: 9.7101s
Epoch: 2 cost time: 1.5372612476348877
Epoch: 2, Steps: 285 | Train Loss: 0.0055747 Vali Loss: 0.0059713 Test Loss: 0.0061728
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.025
	iters: 100, epoch: 3 | loss: 0.0050572
	speed: 0.0203s/iter; left time: 44.3436s
	iters: 200, epoch: 3 | loss: 0.0042581
	speed: 0.0042s/iter; left time: 8.7961s
Epoch: 3 cost time: 1.5056350231170654
Epoch: 3, Steps: 285 | Train Loss: 0.0054441 Vali Loss: 0.0050939 Test Loss: 0.0052185
Validation loss decreased (0.005193 --> 0.005094).  Saving model ...
Updating learning rate to 0.0125
	iters: 100, epoch: 4 | loss: 0.0047074
	speed: 0.0195s/iter; left time: 36.9950s
	iters: 200, epoch: 4 | loss: 0.0044296
	speed: 0.0042s/iter; left time: 7.5322s
Epoch: 4 cost time: 1.4752163887023926
Epoch: 4, Steps: 285 | Train Loss: 0.0052927 Vali Loss: 0.0052015 Test Loss: 0.0052713
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00625
	iters: 100, epoch: 5 | loss: 0.0052940
	speed: 0.0200s/iter; left time: 32.2146s
	iters: 200, epoch: 5 | loss: 0.0050879
	speed: 0.0042s/iter; left time: 6.3174s
Epoch: 5 cost time: 1.5222458839416504
Epoch: 5, Steps: 285 | Train Loss: 0.0052748 Vali Loss: 0.0049438 Test Loss: 0.0050251
Validation loss decreased (0.005094 --> 0.004944).  Saving model ...
Updating learning rate to 0.003125
	iters: 100, epoch: 6 | loss: 0.0072714
	speed: 0.0203s/iter; left time: 26.9076s
	iters: 200, epoch: 6 | loss: 0.0057269
	speed: 0.0042s/iter; left time: 5.1694s
Epoch: 6 cost time: 1.5112664699554443
Epoch: 6, Steps: 285 | Train Loss: 0.0052280 Vali Loss: 0.0049405 Test Loss: 0.0050267
Validation loss decreased (0.004944 --> 0.004941).  Saving model ...
Updating learning rate to 0.0015625
	iters: 100, epoch: 7 | loss: 0.0049033
	speed: 0.0200s/iter; left time: 20.8465s
	iters: 200, epoch: 7 | loss: 0.0048712
	speed: 0.0044s/iter; left time: 4.1099s
Epoch: 7 cost time: 1.5459728240966797
Epoch: 7, Steps: 285 | Train Loss: 0.0052005 Vali Loss: 0.0049575 Test Loss: 0.0050157
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00078125
	iters: 100, epoch: 8 | loss: 0.0054976
	speed: 0.0194s/iter; left time: 14.6435s
	iters: 200, epoch: 8 | loss: 0.0055449
	speed: 0.0044s/iter; left time: 2.8944s
Epoch: 8 cost time: 1.4786503314971924
Epoch: 8, Steps: 285 | Train Loss: 0.0051939 Vali Loss: 0.0049530 Test Loss: 0.0050167
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.000390625
	iters: 100, epoch: 9 | loss: 0.0045471
	speed: 0.0202s/iter; left time: 9.5017s
	iters: 200, epoch: 9 | loss: 0.0041378
	speed: 0.0042s/iter; left time: 1.5665s
Epoch: 9 cost time: 1.5897064208984375
Epoch: 9, Steps: 285 | Train Loss: 0.0051853 Vali Loss: 0.0049415 Test Loss: 0.0050162
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ship_18_18_DLinear_Ship_ftM_sl18_ll18_pl18_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1453
[[0.0864908  0.8636174 ]
 [0.0954706  0.87780917]
 [0.106468   0.88438815]
 [0.119764   0.8922796 ]
 [0.13437293 0.89518064]
 [0.14970186 0.8972486 ]
 [0.16412944 0.9062313 ]
 [0.17896937 0.8973177 ]
 [0.1937748  0.88764447]
 [0.20894307 0.87962294]
 [0.22384395 0.87064415]
 [0.23524587 0.8514789 ]
 [0.24669714 0.8322804 ]
 [0.2565022  0.81144065]
 [0.2648536  0.789767  ]
 [0.2719173  0.76927584]
 [0.27563545 0.74729425]
 [0.2760369  0.7250182 ]
 [0.27583644 0.7013057 ]
 [0.27599618 0.6772128 ]
 [0.2760582  0.6530059 ]
 [0.2744008  0.6289441 ]
 [0.26899737 0.60642636]
 [0.2647692  0.5832648 ]
 [0.26154256 0.559532  ]
 [0.25770494 0.5357985 ]
 [0.25376603 0.51222646]
 [0.25240865 0.4873837 ]
 [0.25146332 0.46229443]
 [0.2507536  0.43695316]
 [0.250009   0.41129258]
 [0.24923058 0.3857308 ]
 [0.2484716  0.36047876]
 [0.24800175 0.3354724 ]
 [0.2426124  0.31341216]
 [0.22941014 0.30180013]]
[[0.0864908  0.8636174 ]
 [0.0954706  0.87780917]
 [0.106468   0.88438815]
 [0.119764   0.8922796 ]
 [0.13437293 0.89518064]
 [0.14970186 0.8972486 ]
 [0.16412944 0.9062313 ]
 [0.17896937 0.8973177 ]
 [0.1937748  0.88764447]
 [0.20894307 0.87962294]
 [0.22384395 0.87064415]
 [0.23524587 0.8514789 ]
 [0.24669714 0.8322804 ]
 [0.2565022  0.81144065]
 [0.2648536  0.789767  ]
 [0.2719173  0.76927584]
 [0.27563545 0.74729425]
 [0.2760369  0.7250182 ]
 [0.28249365 0.7139506 ]
 [0.29526612 0.7162849 ]
 [0.30163622 0.7028096 ]
 [0.31118286 0.6910957 ]
 [0.3202676  0.68086076]
 [0.32786506 0.67232037]
 [0.33492362 0.66200143]
 [0.3425697  0.65074337]
 [0.35030806 0.6402593 ]
 [0.35740948 0.6294865 ]
 [0.36637333 0.6227002 ]
 [0.37292224 0.6176126 ]
 [0.37810466 0.6039916 ]
 [0.3871662  0.6007452 ]
 [0.3892863  0.5803714 ]
 [0.39463556 0.5718296 ]
 [0.399779   0.5614966 ]
 [0.40640318 0.5524529 ]]
[[1.31999996e-05 8.30104828e-01]
 [3.97680001e-03 8.49834442e-01]
 [1.05554396e-02 8.67532194e-01]
 [1.76665597e-02 8.84766757e-01]
 [2.85240803e-02 8.90642047e-01]
 [4.10649553e-02 8.88915360e-01]
 [5.35061210e-02 8.87716532e-01]
 [6.59867972e-02 8.86252940e-01]
 [7.83061832e-02 8.84750962e-01]
 [9.03709754e-02 8.84214580e-01]
 [1.02253877e-01 8.86112452e-01]
 [1.14011385e-01 8.88963640e-01]
 [1.25773877e-01 8.92172098e-01]
 [1.37770981e-01 8.95420432e-01]
 [1.49892807e-01 8.98588896e-01]
 [1.62165672e-01 9.01493728e-01]
 [1.74173206e-01 8.96821082e-01]
 [1.85642675e-01 8.91174674e-01]
 [1.97295636e-01 8.84557664e-01]
 [2.09001362e-01 8.79337132e-01]
 [2.20543325e-01 8.73283863e-01]
 [2.29853198e-01 8.59740734e-01]
 [2.38656834e-01 8.44255447e-01]
 [2.47713193e-01 8.29993725e-01]
 [2.55548894e-01 8.13401341e-01]
 [2.62802094e-01 7.97333181e-01]
 [2.70104676e-01 7.81354427e-01]
 [2.77708143e-01 7.64223516e-01]
 [2.85495788e-01 7.46707201e-01]
 [2.93462843e-01 7.29568660e-01]
 [3.00606787e-01 7.12821126e-01]
 [3.07360590e-01 6.95393503e-01]
 [3.14803213e-01 6.78861201e-01]
 [3.23868006e-01 6.65957928e-01]
 [3.34613889e-01 6.58559144e-01]
 [3.46175641e-01 6.53587103e-01]]
[[1.31999996e-05 8.30104828e-01]
 [3.97680001e-03 8.49834442e-01]
 [1.05554396e-02 8.67532194e-01]
 [1.76665597e-02 8.84766757e-01]
 [2.85240803e-02 8.90642047e-01]
 [4.10649553e-02 8.88915360e-01]
 [5.35061210e-02 8.87716532e-01]
 [6.59867972e-02 8.86252940e-01]
 [7.83061832e-02 8.84750962e-01]
 [9.03709754e-02 8.84214580e-01]
 [1.02253877e-01 8.86112452e-01]
 [1.14011385e-01 8.88963640e-01]
 [1.25773877e-01 8.92172098e-01]
 [1.37770981e-01 8.95420432e-01]
 [1.49892807e-01 8.98588896e-01]
 [1.62165672e-01 9.01493728e-01]
 [1.74173206e-01 8.96821082e-01]
 [1.85642675e-01 8.91174674e-01]
 [1.95233405e-01 8.85228992e-01]
 [2.08102569e-01 8.89943361e-01]
 [2.16885254e-01 8.77601564e-01]
 [2.28974640e-01 8.69587898e-01]
 [2.40752578e-01 8.62753749e-01]
 [2.50641137e-01 8.56234074e-01]
 [2.60654479e-01 8.47089469e-01]
 [2.70747721e-01 8.37231338e-01]
 [2.81060845e-01 8.29207063e-01]
 [2.90037513e-01 8.20003331e-01]
 [3.01014066e-01 8.14034700e-01]
 [3.09026599e-01 8.10850203e-01]
 [3.17510605e-01 8.00244570e-01]
 [3.28471839e-01 8.00156057e-01]
 [3.34554493e-01 7.82646179e-01]
 [3.42721343e-01 7.76791930e-01]
 [3.50415260e-01 7.68988609e-01]
 [3.60025585e-01 7.63498187e-01]]
[[9.8671401e-01 4.1222223e-04]
 [9.8665279e-01 2.6140001e-02]
 [9.8358321e-01 5.0198890e-02]
 [9.7163874e-01 6.3665256e-02]
 [9.5997757e-01 7.6138057e-02]
 [9.4861478e-01 8.8462964e-02]
 [9.3795228e-01 1.0037653e-01]
 [9.2815322e-01 1.1176482e-01]
 [9.1989881e-01 1.2214906e-01]
 [9.1322517e-01 1.3841556e-01]
 [9.0804517e-01 1.6069999e-01]
 [8.9981723e-01 1.8005639e-01]
 [8.9052254e-01 1.9797350e-01]
 [8.8113922e-01 2.1597964e-01]
 [8.7159103e-01 2.3380432e-01]
 [8.6230081e-01 2.5164074e-01]
 [8.5299951e-01 2.6962006e-01]
 [8.4359682e-01 2.8764185e-01]
 [8.3406478e-01 3.0548814e-01]
 [8.2408082e-01 3.2260925e-01]
 [8.1283534e-01 3.3755594e-01]
 [8.0374259e-01 3.5558763e-01]
 [7.9548013e-01 3.7460366e-01]
 [7.8653854e-01 3.9203450e-01]
 [7.7608317e-01 4.0626666e-01]
 [7.6472950e-01 4.1849041e-01]
 [7.5234115e-01 4.2861557e-01]
 [7.3926699e-01 4.3744946e-01]
 [7.2591323e-01 4.4581819e-01]
 [7.1256918e-01 4.5427778e-01]
 [6.9931519e-01 4.6303630e-01]
 [6.8614417e-01 4.7159603e-01]
 [6.7298722e-01 4.8001480e-01]
 [6.5990436e-01 4.8830462e-01]
 [6.4699030e-01 4.9640092e-01]
 [6.3383079e-01 5.0473595e-01]]
[[9.8671401e-01 4.1222223e-04]
 [9.8665279e-01 2.6140001e-02]
 [9.8358321e-01 5.0198890e-02]
 [9.7163874e-01 6.3665256e-02]
 [9.5997757e-01 7.6138057e-02]
 [9.4861478e-01 8.8462964e-02]
 [9.3795228e-01 1.0037653e-01]
 [9.2815322e-01 1.1176482e-01]
 [9.1989881e-01 1.2214906e-01]
 [9.1322517e-01 1.3841556e-01]
 [9.0804517e-01 1.6069999e-01]
 [8.9981723e-01 1.8005639e-01]
 [8.9052254e-01 1.9797350e-01]
 [8.8113922e-01 2.1597964e-01]
 [8.7159103e-01 2.3380432e-01]
 [8.6230081e-01 2.5164074e-01]
 [8.5299951e-01 2.6962006e-01]
 [8.4359682e-01 2.8764185e-01]
 [8.3192855e-01 3.0146497e-01]
 [8.3215690e-01 3.1676409e-01]
 [8.1655318e-01 3.2548982e-01]
 [8.0468678e-01 3.3795601e-01]
 [7.9421866e-01 3.4992144e-01]
 [7.8372443e-01 3.6074704e-01]
 [7.7267450e-01 3.6977893e-01]
 [7.5921744e-01 3.8046825e-01]
 [7.4962413e-01 3.8936427e-01]
 [7.3687351e-01 3.9977562e-01]
 [7.2986394e-01 4.0915191e-01]
 [7.2343397e-01 4.2041379e-01]
 [7.1074277e-01 4.2883453e-01]
 [7.0804036e-01 4.4067961e-01]
 [6.8810284e-01 4.4634652e-01]
 [6.8007827e-01 4.5629370e-01]
 [6.6918164e-01 4.6563637e-01]
 [6.6374189e-01 4.7286093e-01]]
mse:0.005026687867939472, mae:0.04743131622672081
